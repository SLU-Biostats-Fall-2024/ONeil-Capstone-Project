---
title: "Finalproject.qmd"
format: html
editor: visual
---

### Ecotourism on Bear Detection in the Khutze Watershed

Introduction

This dataset is associated with an experiment that tests the effects of human activity and bear viewing on grizzly bear activity in the Khutze watershed. They also observed and noted the trade-off of risk avoidance and foraging associated with salmon levels. Wildlife avoids people and they wanted to highlight this through the bears from forty different cameras.Â  They took the opportunity to test this experiment during the covid 19 pandemic as human interference would be significantly lower as areas were closed to tourism. This is a large data set as it has 4500 rows of data with 20 variables being measured.

To prepare this dataset, very little extra steps were needed. The dataset was had all complete cases, so nothing was left as null or as NA. The only preparation needed was moving the text the right side of the column instead of leaving it in the center and removing the dashes in the column titles.

## Analysis one

This is to see the relationship between the what is the effect of site closure on bear abundance. The null hypothesis is that there is no correlation between site closure and bear abundance.

Set things up

```{r}
rm(list=ls())
library(here)
library(tidyverse)
library(ggfortify)
library(multcomp)
bears <- read_csv(here::here("data/Short_et_al_Data1 (1).csv"))
```

Take a look at our data and see if things will need to be fixed

```{r}
glimpse(bears)
summary(bears)

```

We have 20 variables with 4577 observations. For this analysis we will be looking at the Year closure variable and the detections variable. We need to change year closure to a factor variable

```{r}
#change character variables to factor variables
bears$YearClosures <- as.factor(bears$YearClosures)
bears$Exposure <- as.factor(bears$Exposure)
bears$PeoplePresent <- as.factor(bears$PeoplePresent)
bears$Treatment <- as.factor(bears$Treatment)

```

Check our levels for treatment. Since there is three levels for treatment; closed to tourism, or open in one of two sections, We need to run an anova test. There does not seem to be any issues with how the data is put in so we can continue with our test.

```{r}
#use the levels function to see if there are mistakes 
levels(bears$Treatment)
```

First we need to plot the relation before analyzing the data

```{r}
#plot the relationship between the detection of bears and the treatment to see the correlation 
ggplot(bears, aes(Detections))+
  geom_histogram()+
  facet_wrap(~Treatment, ncol = 1)
```

From this, it can be guessed that there will be a difference from closed to open tourism but not a difference in the two sections that were open to tourism.

```{r}
#making a model
bear.model <- lm(Detections ~ Treatment, data = bears)
autoplot(bear.model)

```

Looking the plots, we can make the assumptions that the data does not follow the normal distribution

```{r}
#run anova to see specific results from the model 
anova(bear.model)
phc1 <- glht(bear.model, linfct = mcp(Treatment = "Tukey"))
summary(phc1)
cld(phc1)
```

We have a p value of less than .05 which shows us that there is a significant difference between the Treatment levels of being open and closed and the amount of bears detected. There is a degrees of freedom of two because we have one minus the treatment levels. Our residuals degrees of freedom is 4574 because we have 4577 observations minus the three treatment levels. (F= 34.742, df= 2, p \< .05) From this test, it shows that there is a significant difference between the open treatments of North and South and closed and but no significant different between the two open treatments.

```{r}
ggplot(bears, aes(x= Treatment, y = Detections)) + geom_point() + geom_smooth(method = 'lm')
```

The figure shows the relationship between the Treatment status of the park such as an open or closed area in relation to how many bears were detected on camera. Results show significant differences with (F= 34.742, df= 2, p \< .05)

### Analysis Two

```{r}
rm(list = ls())
library(here)
library(tidyverse)
library(ggfortify)
library(performance) #for checking model performance
library(broom) #for tidying regression output
library(leaps) #allows best subsets linear regression
library(MASS) #for stepAIC function
library(data.table) #for confidence intervals
library(bestglm)
bears <- read_csv("~/Desktop/OneDrive - St. Lawrence University/Biostats/R_projects/ONeil-Capstone-Project/data/Short_et_al_Data1 (1).csv")
```

### Plot response variable

```{r}
ggplot(bears, aes(Detections))+
  geom_histogram()
range(bears$Detections, na.rm = T)
```

Seeing this graph, there are alot of zero values which may cause trouble in our data analysis

### Examine predictors

Step 2 - Evaluate predictor variables.

We now have the data set set up so that columns 2:6 represent possible predictor variables. Now our task is to see if any of those predictor variables are highly correlated with one another. We don't want to use predictor variables in a model when they are highly correlated. The rule of thumb is that variables with correlation coefficients \> 0.7 (positive or negative) are too highly correlated.

Make new data set with only my predictor variables

```{r}
bears1 <- bears %>% dplyr::select(Detections, SalmonBiomass, Tours, DistanceToTour, DaysSincePeople, tourLength)
```

Now colums 2:6 are numeric predictors. Let's take a look at how correlated they are.

```{r}
cor_tests <- cor(bears1[,2:6], method = "pearson")
cor_tests <- round(cor_tests, 2) #round for easier viewing
```

Tours and tour length are highly correlated with a .95 correlation coefficient which is way to high so I am going to drop tours

```{r}
bears1 <- bears1 %>% dplyr::select(-Tours)
```

Guesstimate predictors

We can take a look at how highly correlated our response variable is with our numeric predictor variables to make a guess as to which predictors might be important and likely to be included in our model.

```{r}
predictor_cors <- data.frame(cor(bears1[,2:5], bears1$Detections))
```

Salmon biomass looks like it could be the best out of all predictors to be correlated as it is the highest number to be closes to 1 but none are strong as they are all low numbers.

Make all the plots

```{r}
ggplot(bears1, aes(SalmonBiomass, Detections))+
  geom_point()
```

```{r}
ggplot(bears1, aes(DistanceToTour, Detections))+
  geom_point()
```

We see that there are a LOT of zeroes for this variable which may be problematic.

```{r}
ggplot(bears1, aes(DaysSincePeople, Detections))+
  geom_point()
```

```{r}
ggplot(bears1, aes(tourLength, Detections))+
  geom_point()
```

### Build Best subsets

Method 1 - Build the model

Now build the models

```{r}
all_subsets.mods <-glm( Detections ~ . , family = poisson, data = bears1)
autoplot(all_subsets.mods)
  
summary(all_subsets.mods)

```

For this plot, the data is not normally distributed. This is because we have too many zero values within our data set. This is called zero inflation. When a large portion of the data is zero it is hard to interpret the data as coefficients and their significants may not be entirely accurate or underestimating the actual effect of predictors. We have to stop our model here as going further with any other model would be past the knowledge of the class.

### Run and Interpret final model

Create the final model

```{r}
final_mod <-glm( Detections ~ SalmonBiomass + tourLength , family = poisson, data = bears1)
summary(final_mod)
anova(final_mod)
```

From our summary call we see that the overall model is highly significant (F = 4.47 with 6 and 68 df, p = 0.00114).

We can see from the coefficients that Shannon diversity is negatively related to Season (with Spring being statistically significant and winter showing borderline significance). There is a positive relationship between the percent of shrubs in the 50-hectare area, but it also has a very modest slope.

We can again use the performance package to check our final model. This gives a nice visual on how well our model fits assumptions. Looks good! (it may give you a message in the console about needing to install package see - choose yes!)

```{r}
library(DHARMa) #package needed to run the check model function
check_model(final_mod)
```

Make a final plot

It would be very challenging to plot this model, as we have 4 variables in the model and we can't plot in 4 dimensional space.

But we could get a couple of plots to look at since we have 4 variables and one is categorical.

To do so, we will use the broom package to tidy up regression results (by putting them into data frames) so that we can more easily work with them.

```{r}
coefs <- tidy(final_mod)
coefs
```

Now get confidence interval

```{r}
ci <- data.table(confint(final_mod), keep.rownames = 'term')
```

Now combine coefs and ci

```{r}
cidf <- cbind(coefs, ci)
cidf
```

```{r}
colnames(cidf)
cidf <-cidf[,-6] #got rid of second term column

cidf <-cidf %>% rename(
  "lower" = "2.5 %",
  "upper" = "97.5 %"
)

cidf$term <- as.factor(cidf$term)
```

Now we can make a plot

```{r}
ggplot(cidf, aes(estimate, term))+
  geom_vline(xintercept = 0, linetype = 2)+
  geom_point(size = 3)+
  geom_errorbarh(aes(xmax = lower, xmin = upper), height = 0.2)+
  theme_bw()
```

This plot shows us the confidence intervals for each term in our model - those that do not include zero for the estimate are statistically significant. You can see that, in this example, when the season is spring, there is a significant negative effect on detected mammal diversity and when the percent shrubs in the habitat increases, there is a signficant and very modest increase in diversity.
