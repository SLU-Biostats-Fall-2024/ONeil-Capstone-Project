---
title: "03. Fit all possible models"
format: html
editor: visual
---

## Fitting the multiple regression models

First, let's clear our workspace and set things up and pull in the data that we cleaned during the last script `02.Examine-the-response-predictor-vars.qmd`. The dataset is called `cleaned_cam_data.csv`. This file corresponds to steps 4 - 6 delineated in `01.multiple-regression-intro.qmd`

### Set things up and pull in the data

```{r}
rm(list = ls())
library(tidyverse)
library(performance) #for checking model performance
library(broom) #for tidying regression output
library(leaps) #allows best subsets linear regression
library(MASS) #for stepAIC function
library(data.table) #for confidence intervals
library(here)
```

Pull in the data

```{r}
cams <- read.csv(here("Data/regression/cleaned_cam_data.csv"))
```

In this file, we're going to look at something called "best subsets regression" which looks at all possible models and determines which models are best. There are a variety of ways to implement best subsets regression â€“ we will use the method from the `leaps` package.

### Step 4 - Fit all possible models

We will look at two methods, one called "best subsets regression" and the other called "stepwise regression".

#### Method 1 - Build your best subsets regressions

We need a matrix that just has our one response variable and our predictor variables, but no other coding variables.

Currently, the variable cam_sd is not a variable we want to consider for our model, so let's drop it.

```{r}
preds <- cams %>% select(-(cam_sd))
```

Now build the models

```{r}
all_subsets.mods <-regsubsets(
  preds$div_shan ~ ., #specifies the model and . tells it to use all predictors
  data = preds,
  nbest = 1 #tells it to pick the one best model for each number of predictors
  )
all_subsets.mods
all_summary <-summary(all_subsets.mods)
outmat<- as.data.frame(all_summary$outmat)
all_summary$adjr2
```

We see from the adjusted R2 values that the model with 8 predictors has the highest R2 value.

We can plot some figures to look at the results

```{r}
plot(all_subsets.mods, scale = "r2") #plots the R^2 value for each variable across all models
```

Here's another way, looking at Mallow's Cp, an index we can use for comparing models.

```{r}
#plotting with base R
plot(all_summary$cp)
plot(all_subsets.mods, scale = "Cp")
```

Be wary of all those negative Mallow's Cp values - they likely suggestion that we have violated an assumption of the test.

Now plot BIC

```{r}
plot(all_summary$bic)
plot(all_subsets.mods, scale = "bic")
```

It looks like the best model is the model with 4 predictors in it. That model has Season-Spring, Season-Winter, mean seedling/sapling/tree density and P_shrub50 as predictors. If we assume Season-Spring and Season-Winter are from the same categorical predictor, Spring, and we then look at a 4th predictor, it would be NumDeer.

#### Method 2 - Stepwise regression

In this method, we start by defining the intercept-only model:

```{r}
m.intercept_only <- glm(preds$div_shan ~ 1, data = preds)
```

Next we define the model with all predictors

```{r}
m.all.preds <- glm(preds$div_shan ~ ., data = preds)
```

Now we perform the stepwise regression to move through.

```{r}
m.stepwise <- step(m.intercept_only, direction = "both", scope = formula(m.all.preds))
```

This method returns a slightly different best model with 4 predictors,but this time they are Season, P_Shrub50, MeanAllDensity and NumDeer. Note that this is the same result as above if we consider Season as a single predictor and not season-spring and season-winter

#### Stepwise with the stepAIC function

Let's try another way to do stepwise with setpAIC method.

First, build the full model

```{r}
full <- glm(div_shan~ ., family = gaussian, data = preds)
summary(full)
```

Now we can begin the stepwise procedure

```{r}
step <- stepAIC(full, trace = F)
step$anova
```

We get a slightly different final model here, with Season, NumDeer, P_Shrub50 like the other models, but P_Deciduous rather than MeanAllDensity as the 4th predictor.

We can compare those final models to one another in this way by first creating each of them

```{r}
mod_best <- lm(div_shan ~ Season + MeanAllDensity + P_Shrub50, data = preds)

mod_step <- lm(div_shan ~ Season + MeanAllDensity + NumDeer + P_Shrub50, data = preds)

mod_stepAIC <- lm(div_shan ~ Season + NumDeer + P_Deciduous50 + P_Shrub50, data = preds)
```

Now we can compare those 3 models

```{r}
AIC(mod_best, mod_step, mod_stepAIC)
```

These models are all within 2 AIC units of one another which means they are more-or-less equivalent.

Let's use the `performance` package to compare these models to one another.

```{r}
performance(mod_best)
performance(mod_step)
performance(mod_stepAIC)

```

mod_step has a slightly higher adjusted R2 so we'll go with it, since these models are more-or-less equivalent. Sigma is a measure of the residual standard error and it measures model accuracy - lower values of RSE are better.

### Steps 5 and - Choose and then run and interpret the best model

According to BIC and adjusted R2, our model with 4 predictors is best. So let's look at that final model.

#### Create the final model

```{r}
final_mod <- lm(div_shan ~ Season + MeanAllDensity + NumDeer + P_Shrub50, data = preds)
summary(final_mod)
anova(final_mod)
```

From our `summary` call we see that the overall model is highly significant (F = 4.47 with 6 and 68 df, p = 0.00114).

We can see from the coefficients that Shannon diversity is negatively related to Season (with Spring being statistically significant and winter showing borderline significance). There is a positive relationship between the percent of shrubs in the 50-hectare area, but it also has a very modest slope.

We can again use the `performance` package to check our final model. This gives a nice visual on how well our model fits assumptions. Looks good! (it may give you a message in the console about needing to install package `see` - choose yes!)

```{r}
check_model(final_mod)
```

#### Make a final plot

It would be very challenging to plot this model, as we have 4 variables in the model and we can't plot in 4 dimensional space.

But we could get a couple of plots to look at since we have 4 variables and one is categorical.

To do so, we will use the `broom` package to tidy up regression results (by putting them into data frames) so that we can more easily work with them.

```{r}
coefs <- tidy(final_mod)
coefs
```

Now get confidence interval

```{r}
ci <- data.table(confint(final_mod), keep.rownames = 'term')
```

Now combine coefs and ci

```{r}
cidf <- cbind(coefs, ci)
cidf
```

```{r}
colnames(cidf)
cidf <-cidf[,-6] #got rid of second term column

cidf <-cidf %>% rename(
  "lower" = "2.5 %",
  "upper" = "97.5 %"
)

cidf$term <- as.factor(cidf$term)
```

Now we can make a plot

```{r}
ggplot(cidf, aes(estimate, term))+
  geom_vline(xintercept = 0, linetype = 2)+
  geom_point(size = 3)+
  geom_errorbarh(aes(xmax = lower, xmin = upper), height = 0.2)+
  theme_bw()
```

This plot shows us the confidence intervals for each term in our model - those that do not include zero for the estimate are statistically significant. You can see that, in this example, when the season is spring, there is a significant negative effect on detected mammal diversity and when the percent shrubs in the habitat increases, there is a signficant and very modest increase in diversity.
